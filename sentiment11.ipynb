{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment11.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNcpDrmY+fxrseEfBeP+v4O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giopnd/notebooks/blob/master/sentiment11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0xmuXTZ7Ujr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "from string import punctuation\n",
        "from random import shuffle\n",
        "\n",
        "import gensim\n",
        "from gensim.models.word2vec import Word2Vec  # the word2vec model gensim class\n",
        "\n",
        "LabeledSentence = gensim.models.doc2vec.LabeledSentence\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "tqdm.pandas(desc=\"progress-bar\")\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "tokenizer = TweetTokenizer()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1LDkNTyyo5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"georgiosgiotis\"\n",
        "os.environ['KAGGLE_KEY'] = \"78e14d9a6090bb989f7240761e76185b\"\n",
        "# Colab library to upload files to notebook\n",
        "from google.colab import files\n",
        "# Install Kaggle library\n",
        "!pip install -q kaggle\n",
        "# Downlaod data\n",
        "!kaggle datasets download -d kazanova/sentiment140\n",
        "# unzip\n",
        "!unzip \"sentiment140.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dN_t3a-7YSS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b79a6e7e-0ea3-44fb-e20a-f32bf3361fb9"
      },
      "source": [
        "def load_dataset():\n",
        "  data = pd.read_csv('./training.1600000.processed.noemoticon.csv', encoding='latin-1', usecols=[0, 5],\n",
        "                     header=None)\n",
        "  data = data[data[0].isnull() == False]\n",
        "  data[0] = data[0].map(int) / 4\n",
        "  data = data[data[5].isnull() == False]\n",
        "  data.reset_index(inplace=True)\n",
        "  data.drop('index', axis=1, inplace=True)\n",
        "  print('dataset loaded with shape', data.shape)\n",
        "  return data\n",
        "\n",
        "\n",
        "data = load_dataset()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset loaded with shape (1600000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoldrPXW7lu9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(tweet):\n",
        "  try:\n",
        "    #tweet = np.unicode(tweet.decode('latin-1').lower())\n",
        "    tokens = tokenizer.tokenize(tweet)\n",
        "    tokens = list(filter(lambda t: not t.startswith('@'), tokens))\n",
        "    tokens = list(filter(lambda t: not t.startswith('#'), tokens))\n",
        "    tokens = list(filter(lambda t: not t.startswith('http'), tokens))\n",
        "    return tokens\n",
        "  except:\n",
        "    return 'NC'\n",
        "\n",
        "\n",
        "def postprocess(data, n=1600000):\n",
        "  data = data\n",
        "  data['tokens'] = data[5].progress_map(tokenize)\n",
        "  data = data[data.tokens != 'NC']\n",
        "  data.reset_index(inplace=True)\n",
        "  data.drop('index', inplace=True, axis=1)\n",
        "  return data\n",
        "\n",
        "\n",
        "data = postprocess(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFvlroE17vMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(np.array(data['tokens']),\n",
        "                                                    np.array(data[0]),\n",
        "                                                    test_size=0.2, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRXcxx0i7wNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def labelizeTweets(tweets, label_type):\n",
        "  labelized = []\n",
        "  for i,v in tqdm(enumerate(tweets)):\n",
        "    label = '%s_%s'%(label_type,i)\n",
        "    labelized.append(LabeledSentence(v, [label]))\n",
        "  return labelized\n",
        "\n",
        "x_train = labelizeTweets(x_train, 'TRAIN')\n",
        "x_test = labelizeTweets(x_test, 'TEST')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuZ6hmTl7zks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
        "\n",
        "model = Word2Vec([x.words for x in x_train], min_count=1,size= 200,workers=3, window =3, sg = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkRWDokG76Cl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model.wv.most_similar('facebook'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1MHZVd7-1Tp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
        "matrix = vectorizer.fit_transform([x.words for x in x_train])\n",
        "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
        "print('vocab size :', len(tfidf))\n",
        "\n",
        "def buildWordVector(tokens, size):\n",
        "    vec = np.zeros(size).reshape((1, size))\n",
        "    count = 0.\n",
        "    for word in tokens:\n",
        "        try:\n",
        "            vec += model[word].reshape((1, size)) * tfidf[word]\n",
        "            count += 1.\n",
        "        except KeyError: # handling the case where the token is not\n",
        "                         # in the corpus. useful for testing.\n",
        "            continue\n",
        "    if count != 0:\n",
        "        vec /= count\n",
        "    return vec\n",
        "\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "n_dim=200\n",
        "train_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in map(lambda x: x.words, x_train)])\n",
        "train_vecs_w2v = scale(train_vecs_w2v)\n",
        "\n",
        "test_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in map(lambda x: x.words, x_test)])\n",
        "test_vecs_w2v = scale(test_vecs_w2v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZJmthgGNefh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(x_train_mod, y_train, features, shuffle, drop, layer1_input, layer2_input, epochs, lr, epsilon, validation):\n",
        "  model_nn = Sequential()\n",
        "  model_nn.add(Dense(layer1_input, input_shape=(features,), activation='relu'))\n",
        "  model_nn.add(Dropout(drop))\n",
        "  model_nn.add(Dense(layer2_input, activation='sigmoid'))\n",
        "  model_nn.add(Dropout(drop))\n",
        "  model_nn.add(Dense(2, activation='softmax'))\n",
        "\n",
        "  optimizer = keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=epsilon, decay=0.0, amsgrad=False)\n",
        "  model_nn.compile(loss='sparse_categorical_crossentropy',\n",
        "                   optimizer=optimizer,\n",
        "                   metrics=['accuracy'])\n",
        "  model_nn.fit(np.array(x_train_mod), y_train,\n",
        "               batch_size=32,\n",
        "               epochs=epochs,\n",
        "               verbose=1,\n",
        "               validation_split=validation,\n",
        "               shuffle=shuffle)\n",
        "  return model_nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruzZhyAHOswI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, SpatialDropout1D, Activation;\n",
        "\n",
        "def model1(x_train, y_train):\n",
        "  features = 2000\n",
        "  shuffle = True\n",
        "  drop = 0.5\n",
        "  layer1_input = 256\n",
        "  layer2_input = 128\n",
        "  epochs = 10\n",
        "  lr = 0.001\n",
        "  epsilon = None\n",
        "  validation = 0.1\n",
        "  x_train_mod = train_vecs_w2v\n",
        "  model = train(x_train_mod, y_train, features, shuffle, drop, layer1_input, layer2_input, epochs, lr, epsilon, validation)\n",
        "  return model\n",
        "\n",
        "model = model1(train_vecs_w2v, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBLNlzIFWdqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_dim=200))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_vecs_w2v, y_train, epochs=3, batch_size=32, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0XnyNzkRD9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(test_vecs_w2v, y_test, batch_size=128, verbose=2)\n",
        "print(score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3-UCIbMnlhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(train_vecs_w2v))\n",
        "print(len(test_vecs_w2v))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}