{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text-rnn0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMqfZTsyHjCnpPP53VkxJNB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giopnd/notebooks/blob/master/text_rnn0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InlBQMYe-p_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "\n",
        "# check english lexicon\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import (\n",
        "    wordnet,\n",
        "    stopwords\n",
        ")\n",
        "\n",
        "# handle regular expressions\n",
        "import re\n",
        "\n",
        "# handle data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, SpatialDropout1D, Activation;\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "#import libraries for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-viNzQuwSrQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"georgiosgiotis\"\n",
        "os.environ['KAGGLE_KEY'] = \"78e14d9a6090bb989f7240761e76185b\"\n",
        "# Colab library to upload files to notebook\n",
        "from google.colab import files\n",
        "# Install Kaggle library\n",
        "!pip install -q kaggle\n",
        "# Downlaod data\n",
        "!kaggle datasets download -d kazanova/sentiment140\n",
        "# unzip\n",
        "!unzip \"sentiment140.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KJrCzBx_NU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding='latin-1', header=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KrFDH9j73Zh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data cleaning\n",
        "def preprocessing_text(df):\n",
        "  # lowercase\n",
        "  df = df.str.lower()\n",
        "  # remove retweets\n",
        "  df = df.str.replace('rt', '')\n",
        "  # remove mentions\n",
        "  df = df.replace(r'@\\w+', '', regex=True)\n",
        "  # remove links\n",
        "  df = df.replace(r'http\\S+', '', regex=True)\n",
        "  df = df.replace(r'www.[^ ]+', '', regex=True)\n",
        "  # remove numbers\n",
        "  df = df.replace(r'[0-9]+', '', regex=True)\n",
        "  # remove special characters and puntuation marks\n",
        "  df = df.replace(r'[!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~]', '', regex=True)\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4USgkF8AlPB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data0 = data[0].apply(lambda x: int(x)/4)\n",
        "data5 = data[5]\n",
        "\n",
        "data5 = preprocessing_text(data5)\n",
        "\n",
        "# split data into training and test dataset\n",
        "def split(dfd, dfl):\n",
        "  x_train, x_test, y_train, y_test = train_test_split(dfd, dfl, test_size=0.2, shuffle=True)\n",
        "  return x_train, x_test, y_train, y_test\n",
        "\n",
        "x_train, x_test, y_train, y_test = split(data5[::64], data0[::64])\n",
        "\n",
        "print(len(x_test))\n",
        "\n",
        "def tokenize_tweets(dataset, features):\n",
        "  tokenization = TfidfVectorizer(max_features=features)\n",
        "  tokenization.fit(dataset)\n",
        "  dataset_transformed = tokenization.transform(dataset).toarray()\n",
        "  return dataset_transformed\n",
        "\n",
        "x_train_mod = tokenize_tweets(x_train, 3500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3_Ho-es6BHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = tfds.features.text.Tokenizer()\n",
        "vocabulary_set = set()\n",
        "for text_tensor in data5:\n",
        "  tokenized = re.sub('[,?.]','', text_tensor).lower().split(' ')\n",
        "  print(tokenized)\n",
        "  #some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
        "  #vocabulary_set.update(some_tokens)\n",
        "vocab_size = len(vocabulary_set)\n",
        "vocab_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KLPsBUjA0Hh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(3500, 64),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='sigmoid'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "#model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "#              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "#              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train_mod, y_train,\n",
        "                    batch_size=64,\n",
        "                    epochs=5,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.1,\n",
        "                    shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}