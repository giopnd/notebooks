{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textClsfLSTM1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOY9diave49c/28aTKlWsG4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giopnd/notebooks/blob/master/textClsfLSTM1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwFv7tahhnIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install tensorflow\n",
        "\n",
        "import csv\n",
        "\n",
        "# check english lexicon\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import (\n",
        "    wordnet,\n",
        "    stopwords\n",
        ")\n",
        "\n",
        "# handle regular expressions\n",
        "import re\n",
        "\n",
        "# handle data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, SpatialDropout1D, Activation;\n",
        "\n",
        "#import libraries for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eRomAfZmPNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reset data structures\n",
        "articles = []\n",
        "labels = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsuHSpthnPUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"georgiosgiotis\"\n",
        "os.environ['KAGGLE_KEY'] = \"78e14d9a6090bb989f7240761e76185b\"\n",
        "# Colab library to upload files to notebook\n",
        "from google.colab import files\n",
        "# Install Kaggle library\n",
        "!pip install -q kaggle\n",
        "# Downlaod data\n",
        "!kaggle datasets download -d kazanova/sentiment140\n",
        "# unzip\n",
        "!unzip \"sentiment140.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuO5DDB6nTQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"training.1600000.processed.noemoticon.csv\", 'r', encoding=\"latin1\") as csvfile:\n",
        "  reader = csv.reader(csvfile, delimiter=',')\n",
        "  next(reader)\n",
        "  try:\n",
        "    for row in reader:\n",
        "      labels.append(int(row[0])/2)\n",
        "      articles.append(row[5])\n",
        "  except UnicodeDecodeError:\n",
        "    pass\n",
        "\n",
        "print(len(articles))\n",
        "print(len(labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJOTwpHudZ66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "step = 4\n",
        "articles_df = pd.DataFrame(articles[::step])\n",
        "labels_df = pd.DataFrame(labels[::step])\n",
        "\n",
        "print(len(articles_df))\n",
        "print(len(labels_df))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiSKlvr8lIaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data cleaning\n",
        "def preprocessing_text(df):\n",
        "  # lowercase\n",
        "  df[0] = df[0].str.lower()\n",
        "  # remove retweets\n",
        "  df[0] = df[0].str.replace('rt', '')\n",
        "  # remove mentions\n",
        "  df[0] = df[0].replace(r'@\\w+', '', regex=True)\n",
        "  # remove links\n",
        "  df[0] = df[0].replace(r'http\\S+', '', regex=True)\n",
        "  df[0] = df[0].replace(r'www.[^ ]+', '', regex=True)\n",
        "  # remove numbers\n",
        "  df[0] = df[0].replace(r'[0-9]+', '', regex=True)\n",
        "  # remove special characters and puntuation marks\n",
        "  df[0] = df[0].replace(r'[!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~]', '', regex=True)\n",
        "  return df\n",
        "\n",
        "articles_df = preprocessing_text(articles_df)\n",
        "print(articles_df[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CD50NjwvFBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "# replace elongated words\n",
        "def in_dict(word):\n",
        "  if wordnet.synsets(word):\n",
        "    return True\n",
        "\n",
        "def replace_elongated_word(word):\n",
        "  regex = r'(\\w*)(\\w+)\\2(\\w*)'\n",
        "  repl = r'\\1\\2\\3'\n",
        "  if in_dict(word):\n",
        "    return word\n",
        "  new_word = re.sub(regex, repl, word)\n",
        "  if new_word != word:\n",
        "    return replace_elongated_word(new_word)\n",
        "  else:\n",
        "    return new_word\n",
        "\n",
        "def detect_elongated_words(row):\n",
        "  regexrep = r'(\\w*)(\\w+)(\\2)(\\w*)'\n",
        "  words = [''.join(i) for i in re.findall(regexrep, row)]\n",
        "  for word in words:\n",
        "    if not in_dict(word):\n",
        "      row = re.sub(word, replace_elongated_word(word), row)\n",
        "  return row\n",
        "\n",
        "articles_df[0] = articles_df[0].apply(lambda x: detect_elongated_words(x))\n",
        "print(articles_df[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJglGgdhROkA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# handle negation with antonyms\n",
        "def replace_antonyms(word):\n",
        "  # get all lemma for word\n",
        "  for syn in wordnet.synsets(word):\n",
        "    for lemma in syn.lemmas():\n",
        "      # if the lemma is an antonyms of word\n",
        "      if lemma.antonyms():\n",
        "        # return antonym\n",
        "        return lemma.antonyms()[0].name()\n",
        "  return word\n",
        "\n",
        "def handling_negation(row):\n",
        "  words = word_tokenize(row)\n",
        "  speach_tags = ['JJ', 'JJR', 'JJS', 'NN', 'VB', 'VBD', 'VBG', 'VBN', 'VBP']\n",
        "  # obtain the type of words\n",
        "  tags = nltk.pos_tag(words)\n",
        "  # ask if we found a negation in words\n",
        "  tags_2 = ''\n",
        "  if \"n't\" in words and \"not\" in words:\n",
        "    tags_2 = tags[min(words.index(\"n't\"), words.index(\"not\")):]\n",
        "    words_2 = words[min(words.index(\"n't\"), words.index(\"not\")):]\n",
        "    words = words[:(min(words.index(\"n't\"), words.index(\"not\")))+1]\n",
        "  elif \"n't\" in words:\n",
        "    tags_2 = tags[words.index(\"n't\"):]\n",
        "    words_2 = words[words.index(\"n't\"):]\n",
        "    words = words[:words.index(\"n't\")+1]\n",
        "  elif \"not\" in words:\n",
        "    tags_2 = tags[words.index(\"not\"):]\n",
        "    words_2 = words[words.index(\"not\"):]\n",
        "    words = words[:words.index(\"not\")+1]\n",
        "  for index, word_tag in enumerate(tags_2):\n",
        "    if word_tag[1] in speach_tags:\n",
        "      words = words+[replace_antonyms(word_tag[0])]+words_2[index+2:]\n",
        "      break\n",
        "  return ' '.join(words)\n",
        "\n",
        "articles_df[0] = articles_df[0].apply(lambda x: handling_negation(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xy1tJ3RMsqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "# remove stop words\n",
        "def stop_words(df):\n",
        "  stop_words_list = stopwords.words('english')\n",
        "  #df[0] = df[0].str.lower()\n",
        "  df[0] = df[0].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words_list)]))\n",
        "  return df\n",
        "\n",
        "articles_df = stop_words(articles_df)\n",
        "print(articles_df[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2Aob-WvNY-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pretty print df\n",
        "with pd.option_context('display.max_rows', 10, 'display.max_columns', None, 'display.max_colwidth', 2000):\n",
        "  print(articles_df.to_string())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv9UTYlXWQ5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split data into training and test dataset\n",
        "def split(dfd, dfl):\n",
        "  x_train, x_test, y_train, y_test = train_test_split(dfd, dfl, test_size=0.2, shuffle=True)\n",
        "  return x_train, x_test, y_train, y_test\n",
        "\n",
        "x_train, x_test, y_train, y_test = split(articles_df[0], labels_df[0])\n",
        "\n",
        "print(len(x_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi6dyavuepe-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert the collection of tweets to a matrix of tf/idf features\n",
        "def tokenize_tweets(dataset, features):\n",
        "  tokenization = TfidfVectorizer(max_features=features)\n",
        "  tokenization.fit(dataset)\n",
        "  dataset_transformed = tokenization.transform(dataset).toarray()\n",
        "  return dataset_transformed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UXGhuK1fvqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the neural network model\n",
        "def train(x_train_mod, y_train, features, shuffle, drop, layer1_input, layer2_input, epochs, lr, epsilon, validation):\n",
        "  model_nn = Sequential()\n",
        "  model_nn.add(Dense(layer1_input, input_shape=(features,), activation='relu'))\n",
        "  model_nn.add(Dropout(drop))\n",
        "  model_nn.add(Dense(layer2_input, activation='sigmoid'))\n",
        "  model_nn.add(Dropout(drop))\n",
        "  model_nn.add(Dense(3, activation='softmax'))\n",
        "\n",
        "  optimizer = keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=epsilon, decay=0.0, amsgrad=False)\n",
        "  model_nn.compile(loss='sparse_categorical_crossentropy',\n",
        "                   optimizer=optimizer,\n",
        "                   metrics=['accuracy'])\n",
        "  model_nn.fit(np.array(x_train_mod), y_train,\n",
        "               batch_size=32,\n",
        "               epochs=epochs,\n",
        "               verbose=1,\n",
        "               validation_split=validation,\n",
        "               shuffle=shuffle)\n",
        "  return model_nn  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opzEeVZegKRR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "ce496ee6-f2c5-4ec1-8e6a-7c10e0992bef"
      },
      "source": [
        "def model1(x_train, y_train):\n",
        "  features = 3500\n",
        "  shuffle = True\n",
        "  drop = 0.5\n",
        "  layer1_input = 512\n",
        "  layer2_input = 256\n",
        "  epochs = 6\n",
        "  lr = 0.001\n",
        "  epsilon = None\n",
        "  validation = 0.1\n",
        "  x_train_mod = tokenize_tweets(x_train, features)\n",
        "  model = train(x_train_mod, y_train, features, shuffle, drop, layer1_input, layer2_input, epochs, lr, epsilon, validation)\n",
        "  return model\n",
        "\n",
        "model = model1(x_train, y_train)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 288000 samples, validate on 32000 samples\n",
            "Epoch 1/6\n",
            "288000/288000 [==============================] - 156s 541us/sample - loss: 0.5132 - acc: 0.7448 - val_loss: 0.4907 - val_acc: 0.7631\n",
            "Epoch 2/6\n",
            "288000/288000 [==============================] - 46s 159us/sample - loss: 0.4820 - acc: 0.7662 - val_loss: 0.4858 - val_acc: 0.7659\n",
            "Epoch 3/6\n",
            "288000/288000 [==============================] - 57s 198us/sample - loss: 0.4644 - acc: 0.7764 - val_loss: 0.4836 - val_acc: 0.7681\n",
            "Epoch 4/6\n",
            "288000/288000 [==============================] - 46s 158us/sample - loss: 0.4448 - acc: 0.7880 - val_loss: 0.4869 - val_acc: 0.7680\n",
            "Epoch 5/6\n",
            "288000/288000 [==============================] - 56s 195us/sample - loss: 0.4235 - acc: 0.8006 - val_loss: 0.4932 - val_acc: 0.7662\n",
            "Epoch 6/6\n",
            "288000/288000 [==============================] - 46s 160us/sample - loss: 0.3999 - acc: 0.8154 - val_loss: 0.5028 - val_acc: 0.7648\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yb8tE3vnHSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()\n",
        "model.to_json()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d20KWTUUOF56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_new = tokenize_tweets(x_train, 3500)\n",
        "new_prediction = model.predict(x_new)\n",
        "\n",
        "labels = ['Negative', 'Neutral', 'Positive']\n",
        "sentiments = [labels[np.argmax(pred)] for pred in new_prediction]\n",
        "#tweet_table_new[\"sentiment\"] = sentiments\n",
        "\n",
        "sizes = [sentiments.count('Negative'), sentiments.count('Neutral'), sentiments.count('Positive')]\n",
        "explode = (0, 0, 0.1)\n",
        "labels = 'Negative', 'Neutral', 'Positive'\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.pie(sizes, explode=explode, colors=\"bwr\", labels=labels, autopct='%1.1f%%',\n",
        "        shadow=True, startangle=90, wedgeprops={'alpha':0.8})\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsVbPJJGssoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model.predict(tokenize_tweets(x_test, 3500)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}