{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textClsfLSTM1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMspNmkY68b27zXcV7sBXpr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giopnd/notebooks/blob/master/textClsfLSTM1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwFv7tahhnIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install tensorflow\n",
        "\n",
        "# check english lexicon\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import (\n",
        "    wordnet,\n",
        "    stopwords\n",
        ")\n",
        "\n",
        "# handle regular expressions\n",
        "import re\n",
        "\n",
        "# handle data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation;\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eRomAfZmPNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reset data structures\n",
        "articles = []\n",
        "labels = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsuHSpthnPUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"georgiosgiotis\"\n",
        "os.environ['KAGGLE_KEY'] = \"78e14d9a6090bb989f7240761e76185b\"\n",
        "# Colab library to upload files to notebook\n",
        "from google.colab import files\n",
        "# Install Kaggle library\n",
        "!pip install -q kaggle\n",
        "# Downlaod data\n",
        "!kaggle datasets download -d kazanova/sentiment140\n",
        "# unzip\n",
        "!unzip \"sentiment140.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuO5DDB6nTQf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e49ae5df-e19a-4969-9390-993531345812"
      },
      "source": [
        "with open(\"training.1600000.processed.noemoticon.csv\", 'r', encoding=\"latin1\") as csvfile:\n",
        "  reader = csv.reader(csvfile, delimiter=',')\n",
        "  next(reader)\n",
        "  try:\n",
        "    for row in reader:\n",
        "      labels.append(row[0])\n",
        "      article = row[5]\n",
        "#      for word in STOPWORDS:\n",
        "#        token = ' ' + word + ' '\n",
        "#        article = article.replace(token, ' ')\n",
        "#        article = article.replace(' ', ' ')\n",
        "      articles.append(article)\n",
        "#      if(len(articles) == 10000):\n",
        "#        break\n",
        "  except UnicodeDecodeError:\n",
        "    pass\n",
        "\n",
        "print(len(articles))\n",
        "print(len(labels))"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1599999\n",
            "1599999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlIy666Tkbz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cols = ['target', 'text']\n",
        "df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='latin-1', skipinitialspace=True)#, usecols=cols)\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJOTwpHudZ66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "articles_df = pd.DataFrame(articles)\n",
        "labels_df = pd.DataFrame(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiSKlvr8lIaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data cleaning\n",
        "def preprocessing_text(df):\n",
        "  # lowercase\n",
        "  df[0] = df[0].str.lower()\n",
        "  # remove retweets\n",
        "  df[0] = df[0].str.replace('rt', '')\n",
        "  # remove mentions\n",
        "  df[0] = df[0].replace(r'@\\w+', '', regex=True)\n",
        "  # remove links\n",
        "  df[0] = df[0].replace(r'http\\S+', '', regex=True)\n",
        "  df[0] = df[0].replace(r'www.[^ ]+', '', regex=True)\n",
        "  # remove numbers\n",
        "  df[0] = df[0].replace(r'[0-9]+', '', regex=True)\n",
        "  # remove special characters and puntuation marks\n",
        "  df[0] = df[0].replace(r'[!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~]', '', regex=True)\n",
        "  return df\n",
        "\n",
        "articles_df = preprocessing_text(articles_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CD50NjwvFBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# replace elongated words\n",
        "def in_dict(word):\n",
        "  if wordnet.synsets(word):\n",
        "    return True\n",
        "\n",
        "def replace_elongated_word(word):\n",
        "  regex = r'(\\w*)(\\w+)\\2(\\w*)'\n",
        "  repl = r'\\1\\2\\3'    \n",
        "  if in_dict(word):\n",
        "    return word\n",
        "  new_word = re.sub(regex, repl, word)\n",
        "  if new_word != word:\n",
        "    return replace_elongated_word(new_word)\n",
        "  else:\n",
        "    return new_word\n",
        "\n",
        "def detect_elongated_words(row):\n",
        "  regexrep = r'(\\w*)(\\w+)(\\2)(\\w*)'\n",
        "  words = [''.join(i) for i in re.findall(regexrep, row)]\n",
        "  for word in words:\n",
        "    if not in_dict(word):\n",
        "      row = re.sub(word, replace_elongated_word(word), row)\n",
        "  return row\n",
        "\n",
        "articles_df[0] = articles_df[0].apply(lambda x: detect_elongated_words(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJglGgdhROkA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# handle negation with antonyms\n",
        "def replace_antonyms(word):\n",
        "  # get all lemma for word\n",
        "  for syn in wordnet.synsets(word):\n",
        "    for lemma in syn.lemmas():\n",
        "      # if the lemma is an antonyms of word\n",
        "      if lemma.antonyms():\n",
        "        # return antonym\n",
        "        return lemma.antonyms()[0].name()\n",
        "  return word\n",
        "\n",
        "def handling_negation(row):\n",
        "  words = word_tokenize(row)\n",
        "  speach_tags = ['JJ', 'JJR', 'JJS', 'NN', 'VB', 'VBD', 'VBG', 'VBN', 'VBP']\n",
        "  # obtain the type of words\n",
        "  tags = nltk.pos_tag(words)\n",
        "  # ask if we found a negation in words\n",
        "  tags_2 = ''\n",
        "  if \"n't\" in words and \"not\" in words:\n",
        "    tags_2 = tags[min(words.index(\"n't\"), words.index(\"not\")):]\n",
        "    words_2 = words[min(words.index(\"n't\"), words.index(\"not\")):]\n",
        "    words = words[:(min(words.index(\"n't\"), words.index(\"not\")))+1]\n",
        "  elif \"n't\" in words:\n",
        "    tags_2 = tags[words.index(\"n't\"):]\n",
        "    words_2 = words[words.index(\"n't\"):] \n",
        "    words = words[:words.index(\"n't\")+1]\n",
        "  elif \"not\" in words:\n",
        "    tags_2 = tags[words.index(\"not\"):]\n",
        "    words_2 = words[words.index(\"not\"):]\n",
        "    words = words[:words.index(\"not\")+1] \n",
        "  \n",
        "  for index, word_tag in enumerate(tags_2):\n",
        "    if word_tag[1] in speach_tags:\n",
        "      words = words+[replace_antonyms(word_tag[0])]+words_2[index+2:]\n",
        "      break\n",
        "\n",
        "  return ' '.join(words)\n",
        "\n",
        "articles_df[0] = articles_df[0].apply(lambda x: handling_negation(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xy1tJ3RMsqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove stop words\n",
        "def stop_words(df):\n",
        "  stop_words_list = stopwords.words('english')\n",
        "  df[0] = df[0].str.lower()\n",
        "  df[0] = df[0].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words_list)]))\n",
        "  return df\n",
        "\n",
        "articles_df = stop_words(articles_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2Aob-WvNY-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pretty print df\n",
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.max_colwidth', 2000):\n",
        "  print(articles_df.to_string())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv9UTYlXWQ5k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "595da1e5-73b0-4190-8e67-c3b9d145ab35"
      },
      "source": [
        "# split data into training and test dataset\n",
        "def split(dfd, dfl):\n",
        "  x_train, x_test, y_train, y_test = train_test_split(dfd, dfl, test_size=0.2, shuffle=True)\n",
        "  return x_train, x_test, y_train, y_test\n",
        "\n",
        "x_train, x_test, y_train, y_test = split(articles_df[0], labels_df[0])\n",
        "\n",
        "print(len(x_test))"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "320000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi6dyavuepe-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert the collection of tweets to a matrix of tf/idf features\n",
        "def tokenize_tweets(dataset, features):\n",
        "  tokenization = TfidfVectorizer(max_features=features)\n",
        "  tokenization.fit(dataset)\n",
        "  dataset_transformed = tokenization.transform(dataset).toarray()\n",
        "  return dataset_transformed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UXGhuK1fvqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the neural network model\n",
        "def train(X_train_mod, y_train, features, shuffle, drop, layer1, layer2, epoch, lr, epsilon, validation):\n",
        "  model_nn = Sequential()\n",
        "  model_nn.add(Dense(layer1, input_shape=(features,), activation='relu'))\n",
        "  model_nn.add(Dropout(drop))\n",
        "  model_nn.add(Dense(layer2, activation='sigmoid'))\n",
        "  model_nn.add(Dropout(drop))\n",
        "  model_nn.add(Dense(3, activation='softmax'))\n",
        "\n",
        "  optimizer = keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=epsilon, decay=0.0, amsgrad=False)\n",
        "  model_nn.compile(loss='sparse_categorical_crossentropy',\n",
        "                   optimizer=optimizer,\n",
        "                   metrics=['accuracy'])\n",
        "  model_nn.fit(np.array(X_train_mod), y_train,\n",
        "               batch_size=32,\n",
        "               epochs=epoch,\n",
        "               verbose=1,\n",
        "               validation_split=validation,\n",
        "               shuffle=shuffle)\n",
        "  return model_nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opzEeVZegKRR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "0037b376-4ba9-4e13-a3bb-69b1662fad39"
      },
      "source": [
        "def model1(x_train, y_train):\n",
        "  features = 435\n",
        "  shuffle = True\n",
        "  drop = 0.5\n",
        "  layer1 = 512\n",
        "  layer2 = 256\n",
        "  epoch = 5\n",
        "  lr = 0.001\n",
        "  epsilon = None\n",
        "  validation = 0.1\n",
        "  x_train_mod = tokenize_tweets(x_train, features)\n",
        "  model = train(x_train_mod, y_train, features, shuffle, drop, layer1, layer2, epoch, lr, epsilon, validation)\n",
        "  return model\n",
        "\n",
        "model1(x_train, y_train)"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1151999 samples, validate on 128000 samples\n",
            "Epoch 1/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-228-7fb29df00173>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-228-7fb29df00173>\u001b[0m in \u001b[0;36mmodel1\u001b[0;34m(x_train, y_train)\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mvalidation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mx_train_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_mod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-227-9ea5211d8b09>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X_train_mod, y_train, features, shuffle, drop, layer1, layer2, epoch, lr, epsilon, validation)\u001b[0m\n\u001b[1;32m     16\u001b[0m                \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                shuffle=shuffle)\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_nn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Received a label value of 4 which is outside the valid range of [0, 3).  Label values: 0 0 0 0 0 0 0 4 0 0 0 4 4 0 4 4 4 4 0 0 0 4 4 4 4 4 4 0 4 0 4 0\n\t [[{{node loss_6/dense_23_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]"
          ]
        }
      ]
    }
  ]
}