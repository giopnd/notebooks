{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textClsfLSTM1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOriBuZmnGnneC0ZPUnT07I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giopnd/notebooks/blob/master/textClsfLSTM1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwFv7tahhnIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install tensorflow\n",
        "\n",
        "# check english lexicon\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import (\n",
        "    wordnet,\n",
        "    stopwords\n",
        ")\n",
        "\n",
        "# handle regular expressions\n",
        "import re\n",
        "\n",
        "# handle data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eRomAfZmPNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reset data structures\n",
        "articles = []\n",
        "labels = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsuHSpthnPUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"georgiosgiotis\"\n",
        "os.environ['KAGGLE_KEY'] = \"78e14d9a6090bb989f7240761e76185b\"\n",
        "# Colab library to upload files to notebook\n",
        "from google.colab import files\n",
        "# Install Kaggle library\n",
        "!pip install -q kaggle\n",
        "# Downlaod data\n",
        "!kaggle datasets download -d kazanova/sentiment140\n",
        "# unzip\n",
        "!unzip \"sentiment140.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuO5DDB6nTQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"training.1600000.processed.noemoticon.csv\", 'r') as csvfile:\n",
        "  reader = csv.reader(csvfile, delimiter=',')\n",
        "  next(reader)\n",
        "  try:\n",
        "    for row in reader:\n",
        "      labels.append(row[0])\n",
        "      article = row[5]\n",
        "      for word in STOPWORDS:\n",
        "        token = ' ' + word + ' '\n",
        "        article = article.replace(token, ' ')\n",
        "        article = article.replace(' ', ' ')\n",
        "      articles.append(article)\n",
        "      if(len(articles) == 100):\n",
        "        break\n",
        "  except UnicodeDecodeError:\n",
        "    pass\n",
        "\n",
        "articles_df = pd.DataFrame(articles)\n",
        "\n",
        "print(len(articles))\n",
        "print(len(labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiSKlvr8lIaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data cleaning\n",
        "def preprocessing_text(df):\n",
        "  # lowercase\n",
        "  df[0] = df[0].str.lower()\n",
        "  # remove retweets\n",
        "  df[0] = df[0].str.replace('rt', '')\n",
        "  # remove mentions\n",
        "  df[0] = df[0].replace(r'@\\w+', '', regex=True)\n",
        "  # remove links\n",
        "  df[0] = df[0].replace(r'http\\S+', '', regex=True)\n",
        "  df[0] = df[0].replace(r'www.[^ ]+', '', regex=True)\n",
        "  # remove numbers\n",
        "  df[0] = df[0].replace(r'[0-9]+', '', regex=True)\n",
        "  # remove special characters and puntuation marks\n",
        "  df[0] = df[0].replace(r'[!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~]', '', regex=True)\n",
        "  return df\n",
        "\n",
        "articles_df = preprocessing_text(articles_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CD50NjwvFBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# replace elongated words\n",
        "def in_dict(word):\n",
        "  if wordnet.synsets(word):\n",
        "    return True\n",
        "\n",
        "def replace_elongated_word(word):\n",
        "  regex = r'(\\w*)(\\w+)\\2(\\w*)'\n",
        "  repl = r'\\1\\2\\3'    \n",
        "  if in_dict(word):\n",
        "    return word\n",
        "  new_word = re.sub(regex, repl, word)\n",
        "  if new_word != word:\n",
        "    return replace_elongated_word(new_word)\n",
        "  else:\n",
        "    return new_word\n",
        "\n",
        "def detect_elongated_words(row):\n",
        "  regexrep = r'(\\w*)(\\w+)(\\2)(\\w*)'\n",
        "  words = [''.join(i) for i in re.findall(regexrep, row)]\n",
        "  for word in words:\n",
        "    if not in_dict(word):\n",
        "      row = re.sub(word, replace_elongated_word(word), row)\n",
        "  return row\n",
        "\n",
        "articles_df[0] = articles_df[0].apply(lambda x: detect_elongated_words(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJglGgdhROkA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# handle negation with antonyms\n",
        "def replace_antonyms(word):\n",
        "  # get all lemma for word\n",
        "  for syn in wordnet.synsets(word):\n",
        "    for lemma in syn.lemmas():\n",
        "      # if the lemma is an antonyms of word\n",
        "      if lemma.antonyms():\n",
        "        # return antonym\n",
        "        return lemma.antonyms()[0].name()\n",
        "  return word\n",
        "\n",
        "def handling_negation(row):\n",
        "  print('b: '+row)\n",
        "  words = word_tokenize(row)\n",
        "  speach_tags = ['JJ', 'JJR', 'JJS', 'NN', 'VB', 'VBD', 'VBG', 'VBN', 'VBP']\n",
        "  # obtain the type of words\n",
        "  tags = nltk.pos_tag(words)\n",
        "  # ask if we found a negation in words\n",
        "  tags_2 = ''\n",
        "  if \"n't\" in words and \"not\" in words:\n",
        "    tags_2 = tags[min(words.index(\"n't\"), words.index(\"not\")):]\n",
        "    words_2 = words[min(words.index(\"n't\"), words.index(\"not\")):]\n",
        "    words = words[:(min(words.index(\"n't\"), words.index(\"not\")))+1]\n",
        "  elif \"n't\" in words:\n",
        "    tags_2 = tags[words.index(\"n't\"):]\n",
        "    words_2 = words[words.index(\"n't\"):] \n",
        "    words = words[:words.index(\"n't\")+1]\n",
        "  elif \"not\" in words:\n",
        "    tags_2 = tags[words.index(\"not\"):]\n",
        "    words_2 = words[words.index(\"not\"):]\n",
        "    words = words[:words.index(\"not\")+1] \n",
        "  \n",
        "  for index, word_tag in enumerate(tags_2):\n",
        "    if word_tag[1] in speach_tags:\n",
        "      words = words+[replace_antonyms(word_tag[0])]+words_2[index+2:]\n",
        "      break\n",
        "\n",
        "  print('a: '+' '.join(words))\n",
        "  return ' '.join(words)\n",
        "\n",
        "articles_df[0] = articles_df[0].apply(lambda x: handling_negation(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xy1tJ3RMsqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove stop words\n",
        "def stop_words(df):\n",
        "  stop_words_list = stopwords.words('english')\n",
        "  df[0] = df[0].str.lower()\n",
        "  df[0] = df[0].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words_list)]))\n",
        "  return df\n",
        "\n",
        "articles_df = stop_words(articles_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2Aob-WvNY-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pretty print df\n",
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.max_colwidth', 2000):\n",
        "  print(articles_df.to_string())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv9UTYlXWQ5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}