{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textClsfLSTM1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN/J2upaVkPdItcIwJK0v0q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giopnd/notebooks/blob/master/textClsfLSTM1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwFv7tahhnIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install tensorflow\n",
        "\n",
        "# check english lexicon\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import (\n",
        "    wordnet,\n",
        "    stopwords\n",
        ")\n",
        "\n",
        "# handle regular expressions\n",
        "import re\n",
        "\n",
        "# handle data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation;\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eRomAfZmPNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reset data structures\n",
        "articles = []\n",
        "labels = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsuHSpthnPUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"georgiosgiotis\"\n",
        "os.environ['KAGGLE_KEY'] = \"78e14d9a6090bb989f7240761e76185b\"\n",
        "# Colab library to upload files to notebook\n",
        "from google.colab import files\n",
        "# Install Kaggle library\n",
        "!pip install -q kaggle\n",
        "# Downlaod data\n",
        "!kaggle datasets download -d kazanova/sentiment140\n",
        "# unzip\n",
        "!unzip \"sentiment140.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuO5DDB6nTQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"training.1600000.processed.noemoticon.csv\", 'r') as csvfile:\n",
        "  reader = csv.reader(csvfile, delimiter=',')\n",
        "  next(reader)\n",
        "  try:\n",
        "    for row in reader:\n",
        "      labels.append(row[0])\n",
        "      article = row[5]\n",
        "      for word in STOPWORDS:\n",
        "        token = ' ' + word + ' '\n",
        "        article = article.replace(token, ' ')\n",
        "        article = article.replace(' ', ' ')\n",
        "      articles.append(article)\n",
        "      if(len(articles) == 10000):\n",
        "        break\n",
        "  except UnicodeDecodeError:\n",
        "    pass\n",
        "\n",
        "print(len(articles))\n",
        "print(len(labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJOTwpHudZ66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "articles_df = pd.DataFrame(articles)\n",
        "labels_df = pd.DataFrame(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiSKlvr8lIaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data cleaning\n",
        "def preprocessing_text(df):\n",
        "  # lowercase\n",
        "  df[0] = df[0].str.lower()\n",
        "  # remove retweets\n",
        "  df[0] = df[0].str.replace('rt', '')\n",
        "  # remove mentions\n",
        "  df[0] = df[0].replace(r'@\\w+', '', regex=True)\n",
        "  # remove links\n",
        "  df[0] = df[0].replace(r'http\\S+', '', regex=True)\n",
        "  df[0] = df[0].replace(r'www.[^ ]+', '', regex=True)\n",
        "  # remove numbers\n",
        "  df[0] = df[0].replace(r'[0-9]+', '', regex=True)\n",
        "  # remove special characters and puntuation marks\n",
        "  df[0] = df[0].replace(r'[!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~]', '', regex=True)\n",
        "  return df\n",
        "\n",
        "articles_df = preprocessing_text(articles_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CD50NjwvFBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# replace elongated words\n",
        "def in_dict(word):\n",
        "  if wordnet.synsets(word):\n",
        "    return True\n",
        "\n",
        "def replace_elongated_word(word):\n",
        "  regex = r'(\\w*)(\\w+)\\2(\\w*)'\n",
        "  repl = r'\\1\\2\\3'    \n",
        "  if in_dict(word):\n",
        "    return word\n",
        "  new_word = re.sub(regex, repl, word)\n",
        "  if new_word != word:\n",
        "    return replace_elongated_word(new_word)\n",
        "  else:\n",
        "    return new_word\n",
        "\n",
        "def detect_elongated_words(row):\n",
        "  regexrep = r'(\\w*)(\\w+)(\\2)(\\w*)'\n",
        "  words = [''.join(i) for i in re.findall(regexrep, row)]\n",
        "  for word in words:\n",
        "    if not in_dict(word):\n",
        "      row = re.sub(word, replace_elongated_word(word), row)\n",
        "  return row\n",
        "\n",
        "articles_df[0] = articles_df[0].apply(lambda x: detect_elongated_words(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJglGgdhROkA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# handle negation with antonyms\n",
        "def replace_antonyms(word):\n",
        "  # get all lemma for word\n",
        "  for syn in wordnet.synsets(word):\n",
        "    for lemma in syn.lemmas():\n",
        "      # if the lemma is an antonyms of word\n",
        "      if lemma.antonyms():\n",
        "        # return antonym\n",
        "        return lemma.antonyms()[0].name()\n",
        "  return word\n",
        "\n",
        "def handling_negation(row):\n",
        "  words = word_tokenize(row)\n",
        "  speach_tags = ['JJ', 'JJR', 'JJS', 'NN', 'VB', 'VBD', 'VBG', 'VBN', 'VBP']\n",
        "  # obtain the type of words\n",
        "  tags = nltk.pos_tag(words)\n",
        "  # ask if we found a negation in words\n",
        "  tags_2 = ''\n",
        "  if \"n't\" in words and \"not\" in words:\n",
        "    tags_2 = tags[min(words.index(\"n't\"), words.index(\"not\")):]\n",
        "    words_2 = words[min(words.index(\"n't\"), words.index(\"not\")):]\n",
        "    words = words[:(min(words.index(\"n't\"), words.index(\"not\")))+1]\n",
        "  elif \"n't\" in words:\n",
        "    tags_2 = tags[words.index(\"n't\"):]\n",
        "    words_2 = words[words.index(\"n't\"):] \n",
        "    words = words[:words.index(\"n't\")+1]\n",
        "  elif \"not\" in words:\n",
        "    tags_2 = tags[words.index(\"not\"):]\n",
        "    words_2 = words[words.index(\"not\"):]\n",
        "    words = words[:words.index(\"not\")+1] \n",
        "  \n",
        "  for index, word_tag in enumerate(tags_2):\n",
        "    if word_tag[1] in speach_tags:\n",
        "      words = words+[replace_antonyms(word_tag[0])]+words_2[index+2:]\n",
        "      break\n",
        "\n",
        "  return ' '.join(words)\n",
        "\n",
        "articles_df[0] = articles_df[0].apply(lambda x: handling_negation(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xy1tJ3RMsqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove stop words\n",
        "def stop_words(df):\n",
        "  stop_words_list = stopwords.words('english')\n",
        "  df[0] = df[0].str.lower()\n",
        "  df[0] = df[0].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words_list)]))\n",
        "  return df\n",
        "\n",
        "articles_df = stop_words(articles_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2Aob-WvNY-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pretty print df\n",
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.max_colwidth', 2000):\n",
        "  print(articles_df.to_string())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv9UTYlXWQ5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split data into training and test dataset\n",
        "def split(dfd, dfl):\n",
        "  x_train, x_test, y_train, y_test = train_test_split(dfd, dfl, test_size=0.2, shuffle=True)\n",
        "  return x_train, x_test, y_train, y_test\n",
        "\n",
        "x_train, x_test, y_train, y_test = split(articles_df[0], labels_df[0])\n",
        "\n",
        "print(len(x_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi6dyavuepe-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert the collection of tweets to a matrix of tf/idf features\n",
        "def tokenize_tweets(dataset, features):\n",
        "  tokenization = TfidfVectorizer(max_features=features)\n",
        "  tokenization.fit(dataset)\n",
        "  dataset_transformed = tokenization.transform(dataset).toarray()\n",
        "  return dataset_transformed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UXGhuK1fvqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the neural network model\n",
        "def train(X_train_mod, y_train, features, shuffle, drop, layer1, layer2, epoch, lr, epsilon, validation):\n",
        "  model_nn = Sequential()\n",
        "  model_nn.add(Dense(layer1, input_shape=(features,), activation='relu'))\n",
        "  model_nn.add(Dropout(drop))\n",
        "  model_nn.add(Dense(layer2, activation='sigmoid'))\n",
        "  model_nn.add(Dropout(drop))\n",
        "  model_nn.add(Dense(3, activation='softmax'))\n",
        "\n",
        "  optimizer = keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=epsilon, decay=0.0, amsgrad=False)\n",
        "  model_nn.compile(loss='sparse_categorical_crossentropy',\n",
        "                   optimizer=optimizer,\n",
        "                   metrics=['accuracy'])\n",
        "  model_nn.fit(np.array(X_train_mod), y_train,\n",
        "               batch_size=32,\n",
        "               epochs=epoch,\n",
        "               verbose=1,\n",
        "               validation_split=validation,\n",
        "               shuffle=shuffle)\n",
        "  return model_nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opzEeVZegKRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model1(x_train, y_train):\n",
        "  features = 435\n",
        "  shuffle = True\n",
        "  drop = 0.5\n",
        "  layer1 = 512\n",
        "  layer2 = 256\n",
        "  epoch = 5\n",
        "  lr = 0.001\n",
        "  epsilon = None\n",
        "  validation = 0.1\n",
        "  x_train_mod = tokenize_tweets(x_train, features)\n",
        "  model = train(x_train_mod, y_train, features, shuffle, drop, layer1, layer2, epoch, lr, epsilon, validation)\n",
        "  return model\n",
        "\n",
        "model1(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}