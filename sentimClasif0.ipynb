{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentimClasif0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMn6OO8+pEwnxVHtqZg1DIT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giopnd/notebooks/blob/master/sentimClasif0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13Aei_Hnr6vw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "import csv\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YacUU7U-r2TY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"georgiosgiotis\"\n",
        "os.environ['KAGGLE_KEY'] = \"78e14d9a6090bb989f7240761e76185b\"\n",
        "# Colab library to upload files to notebook\n",
        "from google.colab import files\n",
        "# Install Kaggle library\n",
        "!pip install -q kaggle\n",
        "# Downlaod data\n",
        "!kaggle datasets download -d kazanova/sentiment140\n",
        "# unzip\n",
        "!unzip \"sentiment140.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICCD2R7ysBiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = {}\n",
        "data[\"sentence\"] = []\n",
        "data[\"polarity\"] = []\n",
        "  \n",
        "with open(\"training.1600000.processed.noemoticon.csv\", 'r', encoding=\"latin1\") as csvfile:\n",
        "  reader = csv.reader(csvfile, delimiter=',')\n",
        "  next(reader)\n",
        "  try:\n",
        "    for row in reader:\n",
        "      data[\"polarity\"].append(int(row[0])/2)\n",
        "      data[\"sentence\"].append(row[5])\n",
        "  except UnicodeDecodeError:\n",
        "    pass\n",
        "\n",
        "\n",
        "mydf = pd.DataFrame.from_dict(data)\n",
        "\n",
        "\n",
        "\n",
        "# print(len(articles))\n",
        "# print(len(labels))\n",
        "\n",
        "# step = 4\n",
        "# articles_df = pd.DataFrame(articles[::step])\n",
        "# labels_df = pd.DataFrame(labels[::step])\n",
        "\n",
        "# print(len(articles_df))\n",
        "# print(len(labels_df))\n",
        "\n",
        "# # split data into training and test dataset\n",
        "# def split(dfd, dfl):\n",
        "#   x_train, x_test, y_train, y_test = train_test_split(dfd, dfl, test_size=0.2, shuffle=True)\n",
        "#   return x_train, x_test, y_train, y_test\n",
        "\n",
        "# x_train, x_test, y_train, y_test = split(articles_df[0], labels_df[0])\n",
        "\n",
        "# print(len(x_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6loHBOVeYsUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# # Load all files from a directory in a DataFrame.\n",
        "# def load_directory_data(directory):\n",
        "#   data = {}\n",
        "#   data[\"sentence\"] = []\n",
        "#   data[\"sentiment\"] = []\n",
        "#   for file_path in os.listdir(directory):\n",
        "#     with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
        "#       data[\"sentence\"].append(f.read())\n",
        "#       data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
        "#   return pd.DataFrame.from_dict(data)\n",
        "\n",
        "\n",
        "# # Merge positive and negative examples, add a polarity column and shuffle.\n",
        "# def load_dataset(directory):\n",
        "#   pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
        "#   neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
        "#   pos_df[\"polarity\"] = 1\n",
        "#   neg_df[\"polarity\"] = 0\n",
        "#   return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# # Download the dataset files.\n",
        "# def download_dataset(force_download=False):\n",
        "#   dataset = tf.keras.utils.get_file(\n",
        "#       fname=\"aclImdb.tar.gz\",\n",
        "#       origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n",
        "#       extract=True)\n",
        "\n",
        "#   return dataset\n",
        "\n",
        "\n",
        "# # Process the dataset files.\n",
        "# def load_datasets(dataset):\n",
        "#   train_df = load_dataset(os.path.join(os.path.dirname(dataset),\n",
        "#                                        \"aclImdb\", \"train\"))\n",
        "#   test_df = load_dataset(os.path.join(os.path.dirname(dataset),\n",
        "#                                       \"aclImdb\", \"test\"))\n",
        "\n",
        "#   return train_df, test_df\n",
        "\n",
        "\n",
        "# dataset_ = download_dataset()\n",
        "# train_df, test_df = load_datasets(dataset_)\n",
        "# train_df.head()\n",
        "\n",
        "# train_df, test_df = x_train, y_train\n",
        "\n",
        "# Training input on the whole training set with no limit on training epochs.\n",
        "train_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(\n",
        "    x_train, y_train, num_epochs=None, shuffle=True)\n",
        "\n",
        "# Prediction on the whole training set.\n",
        "predict_train_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(\n",
        "    x_train, y_train, shuffle=False)\n",
        "# Prediction on the test set.\n",
        "predict_test_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(\n",
        "    x_test, y_test, shuffle=False)\n",
        "\n",
        "embedded_text_feature_column = hub.text_embedding_column(\n",
        "    key=\"sentence\",\n",
        "    module_spec=\"https://tfhub.dev/google/nnlm-en-dim128/1\")\n",
        "\n",
        "estimator = tf.estimator.DNNClassifier(\n",
        "    hidden_units=[500, 100],\n",
        "    feature_columns=[embedded_text_feature_column],\n",
        "    n_classes=2,\n",
        "    optimizer=tf.keras.optimizers.Adagrad(lr=0.003))\n",
        "\n",
        "# Training for 5,000 steps means 640,000 training examples with the default\n",
        "# batch size. This is roughly equivalent to 25 epochs since the training dataset\n",
        "# contains 25,000 examples.\n",
        "estimator.train(input_fn=train_input_fn, steps=5000);\n",
        "\n",
        "train_eval_result = estimator.evaluate(input_fn=predict_train_input_fn)\n",
        "test_eval_result = estimator.evaluate(input_fn=predict_test_input_fn)\n",
        "\n",
        "print(\"Training set accuracy: {accuracy}\".format(**train_eval_result))\n",
        "print(\"Test set accuracy: {accuracy}\".format(**test_eval_result))\n",
        "\n",
        "# query = tf.data.Dataset.from_tensors(('what a nice movie!!'))\n",
        "\n",
        "# predict_x = {\n",
        "#   'what a nice movie!!'\n",
        "# }\n",
        "\n",
        "# def input_fn(features, batch_size=256):\n",
        "#   \"\"\"An input function for prediction.\"\"\"\n",
        "#   # Convert the inputs to a Dataset without labels.\n",
        "#   return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)\n",
        "\n",
        "# predictions = estimator.predict(query)\n",
        "#     #input_fn=lambda: input_fn(predict_x))\n",
        "\n",
        "# for pred_dict in predictions:\n",
        "#   class_id = pred_dict['class_ids'][0]\n",
        "#   probability = pred_dict['probabilities'][class_id]\n",
        "\n",
        "#   print('Prediction is \"{}\" ({:.1f}%)'.format(\n",
        "#       class_id, 100 * probability))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHNmIEHM09Ds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load all files from a directory in a DataFrame.\n",
        "def load_directory_data(directory):\n",
        "  data = {}\n",
        "  data[\"sentence\"] = []\n",
        "  data[\"sentiment\"] = []\n",
        "  for file_path in os.listdir(directory):\n",
        "    with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
        "      data[\"sentence\"].append(f.read())\n",
        "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
        "  return pd.DataFrame.from_dict(data)\n",
        "\n",
        "\n",
        "# Merge positive and negative examples, add a polarity column and shuffle.\n",
        "def load_dataset(directory):\n",
        "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
        "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
        "  pos_df[\"polarity\"] = 1\n",
        "  neg_df[\"polarity\"] = 0\n",
        "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Download the dataset files.\n",
        "def download_dataset(force_download=False):\n",
        "  dataset = tf.keras.utils.get_file(\n",
        "      fname=\"aclImdb.tar.gz\",\n",
        "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n",
        "      extract=True)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "\n",
        "# Process the dataset files.\n",
        "def load_datasets(dataset):\n",
        "  train_df = load_dataset(os.path.join(os.path.dirname(dataset),\n",
        "                                       \"aclImdb\", \"train\"))\n",
        "  test_df = load_dataset(os.path.join(os.path.dirname(dataset),\n",
        "                                      \"aclImdb\", \"test\"))\n",
        "\n",
        "  return train_df, test_df\n",
        "\n",
        "\n",
        "dataset_ = download_dataset()\n",
        "train_df, test_df = load_datasets(dataset_)\n",
        "\n",
        "print(type(train_df))\n",
        "print(type(test_df))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}