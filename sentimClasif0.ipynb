{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentimClasif0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMeS2pM7I0kGJ+0MsMem5vp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giopnd/notebooks/blob/master/sentimClasif0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6loHBOVeYsUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "# Load all files from a directory in a DataFrame.\n",
        "def load_directory_data(directory):\n",
        "  data = {}\n",
        "  data[\"sentence\"] = []\n",
        "  data[\"sentiment\"] = []\n",
        "  for file_path in os.listdir(directory):\n",
        "    with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
        "      data[\"sentence\"].append(f.read())\n",
        "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
        "  return pd.DataFrame.from_dict(data)\n",
        "\n",
        "\n",
        "# Merge positive and negative examples, add a polarity column and shuffle.\n",
        "def load_dataset(directory):\n",
        "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
        "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
        "  pos_df[\"polarity\"] = 1\n",
        "  neg_df[\"polarity\"] = 0\n",
        "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Download the dataset files.\n",
        "def download_dataset(force_download=False):\n",
        "  dataset = tf.keras.utils.get_file(\n",
        "      fname=\"aclImdb.tar.gz\",\n",
        "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n",
        "      extract=True)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "\n",
        "# Process the dataset files.\n",
        "def load_datasets(dataset):\n",
        "  train_df = load_dataset(os.path.join(os.path.dirname(dataset),\n",
        "                                       \"aclImdb\", \"train\"))\n",
        "  test_df = load_dataset(os.path.join(os.path.dirname(dataset),\n",
        "                                      \"aclImdb\", \"test\"))\n",
        "\n",
        "  return train_df, test_df\n",
        "\n",
        "\n",
        "dataset_ = download_dataset()\n",
        "train_df, test_df = load_datasets(dataset_)\n",
        "# train_df.head()\n",
        "\n",
        "# Training input on the whole training set with no limit on training epochs.\n",
        "train_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(\n",
        "    train_df, train_df[\"polarity\"], num_epochs=None, shuffle=True)\n",
        "\n",
        "# Prediction on the whole training set.\n",
        "predict_train_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(\n",
        "    train_df, train_df[\"polarity\"], shuffle=False)\n",
        "# Prediction on the test set.\n",
        "predict_test_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(\n",
        "    test_df, test_df[\"polarity\"], shuffle=False)\n",
        "\n",
        "embedded_text_feature_column = hub.text_embedding_column(\n",
        "    key=\"sentence\",\n",
        "    module_spec=\"https://tfhub.dev/google/nnlm-en-dim128/1\")\n",
        "\n",
        "estimator = tf.estimator.DNNClassifier(\n",
        "    hidden_units=[500, 100],\n",
        "    feature_columns=[embedded_text_feature_column],\n",
        "    n_classes=2,\n",
        "    optimizer=tf.keras.optimizers.Adagrad(lr=0.003))\n",
        "\n",
        "# Training for 5,000 steps means 640,000 training examples with the default\n",
        "# batch size. This is roughly equivalent to 25 epochs since the training dataset\n",
        "# contains 25,000 examples.\n",
        "estimator.train(input_fn=train_input_fn, steps=5000);\n",
        "\n",
        "train_eval_result = estimator.evaluate(input_fn=predict_train_input_fn)\n",
        "test_eval_result = estimator.evaluate(input_fn=predict_test_input_fn)\n",
        "\n",
        "print(\"Training set accuracy: {accuracy}\".format(**train_eval_result))\n",
        "print(\"Test set accuracy: {accuracy}\".format(**test_eval_result))\n",
        "\n",
        "query = tf.data.Dataset.from_tensors(('what a nice movie!!'))\n",
        "\n",
        "predict_x = {\n",
        "  'what a nice movie!!'\n",
        "}\n",
        "\n",
        "def input_fn(features, batch_size=256):\n",
        "  \"\"\"An input function for prediction.\"\"\"\n",
        "  # Convert the inputs to a Dataset without labels.\n",
        "  return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)\n",
        "\n",
        "predictions = estimator.predict(query)\n",
        "    #input_fn=lambda: input_fn(predict_x))\n",
        "\n",
        "for pred_dict in predictions:\n",
        "  class_id = pred_dict['class_ids'][0]\n",
        "  probability = pred_dict['probabilities'][class_id]\n",
        "\n",
        "  print('Prediction is \"{}\" ({:.1f}%)'.format(\n",
        "      class_id, 100 * probability))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}